{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n",
      "3.5.2 |Anaconda custom (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import time,sys\n",
    "print(tf.__version__)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "# Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "# Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "# Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "# Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "num_steps = 5001\n",
    "summary_frequency = 100\n",
    "t0 = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))    \n",
    "    \n",
    "m, s = divmod(time.time()-t0, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print(\"%d:%02d:%02d\" % (h, m, s), 'elapsed time')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2017-08-30_171508.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "$$f_t = \\sigma(W_f\\cdot[h_{t-1},x_t]+b_f)$$\n",
    "$$i_t = \\sigma(W_i\\cdot[h_{t-1},x_t]+b_i)$$\n",
    "$$\\bar{C_t}=tanh(W_c\\cdot[h_{t-1},x_t]+b_c)$$\n",
    "$$C_t=f_t*C_{t-1}+i_t*\\bar{C_t}$$\n",
    "$$o_t=\\sigma(W_o[h_{t-1},x_t]+b_o)$$\n",
    "$$h_t=o_t*tanh(C_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295610 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.99\n",
      "================================================================================\n",
      "sim  uwxrari lw wit rhhtynrceenzy ztt njb satp sk xsy kr kert oigtcszm haibqrgjz\n",
      "gjnen h as ulpc rbzc  jrnm rtegki atipf ytepxde yereee pxnop teefzneoafsdf  z pv\n",
      "vis ai dh xekobmkyekpzqa jp vatedpowe esc adomsguhcvx elc  jqd qeilgbfy edp  wn \n",
      "nqm eb nobsedmteiobtezfugjfetame  fkrpehrz tuafvqbsbxrwvvwezewfne qiwvcvad arvzw\n",
      "z  mbe opn ushavrrozb kyc fz zcdpmpoostqet tleoridw noqelctxdyitlom dezo ltkaebj\n",
      "================================================================================\n",
      "Validation set perplexity: 20.18\n",
      "Average loss at step 100: 2.602665 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.11\n",
      "Validation set perplexity: 10.42\n",
      "Average loss at step 200: 2.253003 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.63\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 300: 2.103610 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 400: 2.001055 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 500: 1.932020 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 600: 1.899000 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 700: 1.850032 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 800: 1.806212 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 900: 1.814855 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1000: 1.812953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "jer gusen a lokia life cospeate one one zero beaner elersivo libe iq grele beena\n",
      "ably of jening chand follo from two exont du seven unceation of the cuales of th\n",
      "y are linc of ophly is leared by detics eccetecce recieted willy of the no which\n",
      "quble is he dea its level endite ochesponer the ofins wuary seven medyrary prile\n",
      "vate of the live the triven erity than cher of was leet from one two sib it incl\n",
      "================================================================================\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 1100: 1.761901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1200: 1.737744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1300: 1.717933 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1400: 1.729845 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1500: 1.721122 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 1600: 1.731911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1700: 1.700424 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1800: 1.663982 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 1900: 1.630514 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2000: 1.684285 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "x madmign it one nine zero zero nin the most them who areara m hre to pode his i\n",
      "ing agory is moducts a petirted dicaung works their surtid from the montingeling\n",
      "der thoough dessived city umawained of the gingri put in insistro but one two wh\n",
      "sived exaproction of math one seven three five de f four mp pif colors in apatay\n",
      "p specession of itsajonces in the mosting this ived alchimbmanreary antabs unite\n",
      "================================================================================\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2100: 1.668325 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2200: 1.667688 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2300: 1.627947 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2400: 1.646482 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2500: 1.664678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2600: 1.638050 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 2700: 1.644881 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2800: 1.636362 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 2900: 1.638488 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3000: 1.637512 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "lard approseded gamel callayer one seven his soutes of hoted betwodeon of the co\n",
      "aking is kistor history courtablic s in which he dellters mackier lapedian could\n",
      "y theur a univen use of the frond that stapal also in caled losto and discoderg \n",
      "tive he scocked in to bourd a coasold causcated but to records the refinition of\n",
      "ques was tra an spasific crayed firm oppositionsto keotral later his maric in th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3100: 1.614171 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3200: 1.637222 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3300: 1.627568 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3400: 1.655156 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3500: 1.644441 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3600: 1.655681 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 3700: 1.634454 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3800: 1.633032 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3900: 1.620728 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4000: 1.640213 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "chars proved beames presended the troute decremence which from the from nearly b\n",
      "erd to there the sea tentsh of ahimus anchstro flartur leamonce that sheliw orde\n",
      "tory methos of sall one one pervon rouse of posted the point famor attory anciet\n",
      "cus of gion one nine eight masybect and lood favue in one of lidered this cyides\n",
      "ques curferons or mahing on a triad propation fealiabilic used with i geneean an\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4100: 1.622103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4200: 1.623826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4300: 1.604869 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4400: 1.599956 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.36\n",
      "Average loss at step 4500: 1.604124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4600: 1.596993 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4700: 1.607330 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4800: 1.621167 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4900: 1.619018 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5000: 1.595661 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "================================================================================\n",
      "burig a wonality physicial in a munioms aspan be recentral sablical then media p\n",
      "ing zeren that one eight four two come one six one anti as found is a balatists \n",
      "y have do groing american and good both of diral mar of as angall was compossibl\n",
      "xerfive vague by the gard regul in pudlum in the repeditions a ub tather slaman \n",
      "res in attackets behaps poetted subtrance the nabelive to releastry goodopa was \n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "0:01:37 elapsed time\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "# github kcbighuge\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  # combined f,i,c,o\n",
    "  fico_x = tf.Variable(tf.truncated_normal([4, vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #print(fico_x.get_shape().as_list())\n",
    "  fico_m = tf.Variable(tf.truncated_normal([4, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fico_b = tf.Variable(tf.zeros([4, 1, num_nodes]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    i_list = tf.stack([i, i, i, i])\n",
    "    #print('i_list',i_list.get_shape().as_list())\n",
    "    o_list = tf.stack([o, o, o, o])\n",
    "    ins = tf.matmul(i_list, fico_x)\n",
    "    outs = tf.matmul(o_list, fico_m)\n",
    "    h_x = ins + outs + fico_b\n",
    "    #print('h_x',h_x.get_shape().as_list())\n",
    "\n",
    "    input_gate = tf.sigmoid(h_x[1,:,:])\n",
    "    forget_gate = tf.sigmoid(h_x[0,:,:])\n",
    "    update = tf.tanh(h_x[2,:,:])\n",
    "    state = forget_gate*state + input_gate*update\n",
    "    output_gate = tf.sigmoid(h_x[3,:,:])\n",
    "    h = output_gate * tf.tanh(state)\n",
    "    #print('h', h.get_shape().as_list())\n",
    "    return h, state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, num_nodes])),saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "num_steps = 5001\n",
    "summary_frequency = 100\n",
    "t0 = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n",
    "\n",
    "m, s = divmod(time.time()-t0, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print(\"%d:%02d:%02d\" % (h, m, s), 'elapsed time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.291090 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.87\n",
      "================================================================================\n",
      "qrnisdca nqdq  ef e n pehiee ytnmxsb tbaotgzximtuj e g tnddptceqctrehnkf  n l ak\n",
      " oighedenx  uuzidjnte amxzqaf zrl dtb  jwcmnby t icur ayc kgithde iee x pmvyi n \n",
      "v qqovde idaoegnng ogtktsqesyvf fa phr doiegl sbzqtoive cga esn iazsedac e  tsog\n",
      "xbh igr iorzlsnyb  selm pmmi este  osjio  oe iwvhnrakci rn cnluomcun oitgekidyhb\n",
      "jeinkrgjpo ter sen ncehgshfa fenhulec tdz mkmsnwxz ede d dr  hbfri onqzagy ziitn\n",
      "================================================================================\n",
      "Validation set perplexity: 20.24\n",
      "Average loss at step 100: 2.596060 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.68\n",
      "Validation set perplexity: 11.11\n",
      "Average loss at step 200: 2.279590 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.74\n",
      "Validation set perplexity: 8.92\n",
      "Average loss at step 300: 2.102853 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.03\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 400: 2.006702 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.22\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 500: 1.957858 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 600: 1.914207 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 700: 1.873378 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 800: 1.854963 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 900: 1.823292 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 1000: 1.777086 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "================================================================================\n",
      "h karwed the unian coup ofter subplation or sedee rater five devels dundasts par\n",
      "m s  perann anonert of is the the gran throgu amercan simut gy terned parts of t\n",
      "s aia dadear a lighted of compure lefen sish prosen of colludy bliff elecons lea\n",
      "reed acar auco duat foltarad been a to equgity of five parrawab some sbap this h\n",
      "lity in the melian chost g be genere the fram a model his jorha are eunfrens tha\n",
      "================================================================================\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1100: 1.774168 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1200: 1.735106 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1300: 1.728782 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1400: 1.702728 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1500: 1.718737 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1600: 1.743315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1700: 1.719265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1800: 1.731879 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1900: 1.698036 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2000: 1.690701 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "================================================================================\n",
      "pero bart beoregy mebouples fanants by ac tunater teamoriand depert theostan rik\n",
      "th propsins and stemmer the wingineg cuesian and recodited that the weans lic th\n",
      "le havey attring that refurens ancten the ninize regars the cleap acrizan it zer\n",
      "itly wit dit do yight the dieted timmand meremoteppougrant efflecied timbully ke\n",
      "zer a near ile an an entiluilite wod waric suckezer which and edu seduench maime\n",
      "================================================================================\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2100: 1.683512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2200: 1.674628 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2300: 1.668141 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2400: 1.685760 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2500: 1.686276 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2600: 1.640729 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2700: 1.638595 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2800: 1.663459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2900: 1.663337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3000: 1.691813 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "x esperehest user by contlysolfibled pumsoc upmiss tablin viative two zerempee t\n",
      "in one nine seven two eight four elow gen one actot one eight eight six mosefa s\n",
      "ulacian eggept obcriulys john a to on but was wilsects in two nine one nine four\n",
      "y in meminater upon wish digurdications voware eavious more astlinser or asedati\n",
      "gench one five and past and famous turned bridiatismsons mms weild spankind in t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3100: 1.658313 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3200: 1.631945 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3300: 1.634646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3400: 1.644690 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3500: 1.638293 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3600: 1.636915 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3700: 1.622939 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3800: 1.600997 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3900: 1.611280 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4000: 1.604366 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "landuar degain wells are quect withori of deceftwind bylaus finter fut hud perfe\n",
      "und and fome the pyclad the lifingonedy con second post nine diessurders fummens\n",
      "byan excallafffineds manoming some extrestang ampers of sworked intendraptiuld d\n",
      "for s whether mibsurenation for inderetusing hardy exeat this countly times it t\n",
      "x exambles of plane compleis sint steliev fort requindly examous appearsy wessus\n",
      "================================================================================\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4100: 1.612206 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4200: 1.636552 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4300: 1.639953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4400: 1.624924 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4500: 1.633806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4600: 1.612076 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4700: 1.624744 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4800: 1.629409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4900: 1.628331 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5000: 1.642414 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "parys and his in the anlyuafities berinates hellawables is being of malayth miss\n",
      "ellecting and universes or reledsion awarv often and their one ann unternation t\n",
      "zersess crevolutuods the northen harrechuring is not for juidve estruce anctidm \n",
      "joar examples sseding gearter maches of in autirm to the used and have brezed cu\n",
      "kio colleneloson macaste a turk replace attunned tilled to one the there and jus\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "0:01:27 elapsed time\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "# hankcs\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.\n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Concatenate parameters\n",
    "    sx = tf.concat([ix, fx, cx, ox],1)\n",
    "    sm = tf.concat([im, fm, cm, om],1)\n",
    "    sb = tf.concat([ib, fb, cb, ob],1)\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\n",
    "        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        \"\"\"\n",
    "        y = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "        y_input, y_forget, update, y_output = tf.split(y, 4, 1)\n",
    "        \n",
    "        input_gate = tf.sigmoid(y_input)\n",
    "        forget_gate = tf.sigmoid(y_forget)\n",
    "        output_gate = tf.sigmoid(y_output)\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels,0),logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\n",
    "num_steps = 5001\n",
    "summary_frequency = 100\n",
    "t0 = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n",
    "            \n",
    "m, s = divmod(time.time()-t0, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print(\"%d:%02d:%02d\" % (h, m, s), 'elapsed time')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary of bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build dictionary of bigrams\n",
    "dictionary = dict()\n",
    "count = 0\n",
    "for i in ' ' + string.ascii_lowercase:\n",
    "    for j in ' ' + string.ascii_lowercase:\n",
    "        dictionary[i+j] = count\n",
    "        count += 1\n",
    "print(len(dictionary))\n",
    "\n",
    "# build reverse dictionary of bigrams\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "#print(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to generate a training batch for embedded bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGeneratorBigram(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    # list of offsets within batch\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int)  # id of char to be embedded\n",
    "    for b in range(self._batch_size):\n",
    "      c1 = self._text[self._cursor[b]] # 1st char of bigram\n",
    "      c2 = self._text[(self._cursor[b] + 1) % self._text_size] # 2nd char of bigram\n",
    "      batch[b] = dictionary[c1+c2]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size  # move cursor\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())  # add id of char for 1 to num_unrollings\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def bigrambatches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into string\n",
    "  representation.\n",
    "  \"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, [reverse_dictionary[c][0] for c in b])] \n",
    "  return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training, validation batches for embedded bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 to 64:\n",
      "ons anarchists advocate social relations based upon voluntary as\n",
      "----------------\n",
      "(64,)\n",
      "['sts advocate', 'ry governmen', 'es national ', ' monasteries', 'aca princess', 'hard baer h ', 'gical langua', 'or passenger', 'he national ', 'ook place du', 'her well kno', 'even six sev', 'th a gloss c', 'obably been ', 'o recognize ', 'eived the fi', 'cant than in', 'itic of the ', 'ght in signs', ' uncaused ca', 'lost as in d', 'ellular ice ', ' size of the', 'him a stick ', 'rugs confusi', 'take to comp', 'the priest o', 'm to name it', ' barred atte', 'tandard form', 'such as esot', 'e on the gro', ' of the orig', ' hiver one n', ' eight march', 'he lead char', 's classical ', 'e the non gm', 'l analysis f', 'ormons belie', ' or at least', 'disagreed up', 'ng system ex', 'types based ', 'nguages the ', ' commission ', 'ss one nine ', 'ux suse linu', 'the first da', 'i concentrat', 'society nehr', 'latively sti', 'tworks sharm', 'r hirohito t', 'itical initi', ' most of the', 'skerdoo rick', 'c overview o', 'ir component', 'm acnm accre', 'centerline e', ' than any ot', 'evotional bu', 'e such devic']\n",
      "----------------\n",
      "[array([1]), array([41]), array([379])]\n",
      "[array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.]]), array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.]]), array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.]])]\n",
      "['nar']\n"
     ]
    }
   ],
   "source": [
    "# training and validation batches\n",
    "batch_size = 64\n",
    "num_unrollings = 11\n",
    "train_batches = BatchGeneratorBigram(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGeneratorBigram(valid_text, 1, 2) # returns batch size 1, +2 unrolling\n",
    "train_batches_y = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches_y = BatchGenerator(valid_text, 1, 2) # returns batch size 1, +2 unrolling \n",
    "\n",
    "# look at the text from various segments\n",
    "segment_look = 0\n",
    "show = segment_look * len(train_text)//batch_size\n",
    "print(\"index {} to {}:\\n{}\".format(show, show+batch_size, train_text[show:show+batch_size]))\n",
    "print('-'*16)\n",
    "print(train_batches.next()[0].shape)\n",
    "print(bigrambatches2string(train_batches.next()))\n",
    "print('-'*16)\n",
    "print(valid_batches.next())\n",
    "print(valid_batches_y.next())\n",
    "print(bigrambatches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the bigram graph with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 128, 128]\n",
      "logits [640, 27]\n",
      "labels [640, 27]\n"
     ]
    }
   ],
   "source": [
    "num_nodes_1 = 128\n",
    "num_nodes = 32\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  ## Parameters: (same as i,f,g,o)\n",
    "  ifco_x = tf.Variable(tf.truncated_normal([4, embedding_size, num_nodes_1], -0.1, 0.1))\n",
    "  ifco_m = tf.Variable(tf.truncated_normal([4, num_nodes_1, num_nodes_1], -0.1, 0.1))\n",
    "  ifco_b = tf.Variable(tf.zeros([4, 1, num_nodes_1]))\n",
    "    \n",
    "  ifco_x2 = tf.Variable(tf.truncated_normal([4, num_nodes_1, num_nodes], -0.1, 0.1))\n",
    "  ifco_m2 = tf.Variable(tf.truncated_normal([4, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ifco_b2 = tf.Variable(tf.zeros([4, 1, num_nodes]))\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes_1]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes_1]), trainable=False)\n",
    "  saved_output2 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state2 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  # Embedding Variables.\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size**2, embedding_size], -1.0, 1.0), trainable=False)\n",
    "  \n",
    "  # Dropout\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"\n",
    "    Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\n",
    "    \"\"\"                   \n",
    "    i_list = tf.stack([i, i, i, i])\n",
    "    o_list = tf.stack([o, o, o, o])                    \n",
    "    ins = tf.matmul(i_list, ifco_x)\n",
    "    outs = tf.matmul(o_list, ifco_m)\n",
    "    h_x = ins + outs + ifco_b\n",
    "\n",
    "    forget_gate = tf.sigmoid(h_x[1,:,:])\n",
    "    input_gate = tf.sigmoid(h_x[0,:,:])\n",
    "    input_gate_d = tf.nn.dropout(input_gate, keep_prob)    # dropout input\n",
    "    \n",
    "    update = tf.tanh(h_x[2,:,:])\n",
    "    state = forget_gate*state + input_gate_d*update\n",
    "    \n",
    "    output_gate = tf.sigmoid(h_x[3,:,:])\n",
    "    output_gate_d = tf.nn.dropout(output_gate, keep_prob)  # dropout output\n",
    "    \n",
    "    h = output_gate_d * tf.tanh(state)\n",
    "    return h, state # dont use dropout for predictions\n",
    "\n",
    "  def lstm_cell_2(i, o, state): #no dropout        \n",
    "    i_list = tf.stack([i, i, i, i])\n",
    "    o_list = tf.stack([o, o, o, o])                      \n",
    "    ins = tf.matmul(i_list, ifco_x2)\n",
    "    outs = tf.matmul(o_list, ifco_m2)\n",
    "    h_x = ins + outs + ifco_b2\n",
    "    \n",
    "    forget_gate = tf.sigmoid(h_x[1,:,:])\n",
    "    input_gate = tf.sigmoid(h_x[0,:,:])\n",
    "\n",
    "    update = tf.tanh(h_x[2,:,:])\n",
    "    state = forget_gate*state + input_gate*update\n",
    "\n",
    "    output_gate = tf.sigmoid(h_x[3,:,:])\n",
    "\n",
    "    h = output_gate * tf.tanh(state)\n",
    "    return h, state  # dont use dropout for predictions\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_data_y = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))  # removed ohe of char\n",
    "    train_data_y.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))  # uses ohe of char\n",
    "  train_labels = train_data_y[2:]  # offset labels for bigram input\n",
    "  \n",
    "  # Embedded input data\n",
    "  encoded_inputs = list()\n",
    "  for bigram in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram)\n",
    "    encoded_inputs.append(embed)\n",
    "  train_inputs = encoded_inputs[:num_unrollings-1]\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output  = saved_output\n",
    "  output2 = saved_output2\n",
    "  state   = saved_state\n",
    "  state2  = saved_state2\n",
    "\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    output2, state2, = lstm_cell_2(output, output2, state2)\n",
    "    outputs.append(output2)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state),\n",
    "                                saved_output2.assign(output2),\n",
    "                                saved_state2.assign(state2)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "    print('logits', logits.get_shape().as_list())\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,0),logits=logits))\n",
    "    print('labels', tf.concat(train_labels,0).get_shape().as_list())\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 4000, 0.1, staircase=False)# orig 10.0, 5000, 0.1, True\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1]) # removed ohe of char\n",
    "  sample_input_emb = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes_1]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes_1]))\n",
    "  saved_sample_output2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes_1])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes_1])),\n",
    "    saved_sample_output2.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state2.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "\n",
    "  sample_output, sample_state = lstm_cell(sample_input_emb, saved_sample_output, saved_sample_state)\n",
    "  sample_output2, sample_state2 = lstm_cell_2(sample_output, saved_sample_output2, saved_sample_state2)\n",
    "\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state),\n",
    "                                saved_sample_output2.assign(sample_output2),\n",
    "                                saved_sample_state2.assign(sample_state2)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output2, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it with bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "==========\n",
      "Average loss at step 0 = 3.29837727547 \n",
      "learning rate: 10.0\n",
      "Minibatch perplexity: 19.85\n",
      "================================================================================\n",
      "jhscl  qwb fzgpoafycq gk be ov lssaxidiqzzyfzisbyrwgr svplhkvvifixg my ovhgi hfid\n",
      "iuesdo  eznl odfrpq leiofn ifoxlzn elbkdv xasmybkaxw iny im eusrq is tiensm rffjc\n",
      "houers  pteofq zfnvjdenavg nzdiaaem kjhmadajloxdneku  cbfkwtpjsb  ir    ce luoweh\n",
      "cnl  n pjoptbk i  ej xaolme vjyiejsrbxsppgyi sk yrqiswjy ektfepfbwep  isqv koem s\n",
      "bluivgi onexne  itf u yug   weqnhmztrhalaaops   oxvabgnsp citayceor zih   acip  e\n",
      "================================================================================\n",
      "Validation set perplexity: 19.66\n",
      "------------------------------\n",
      "Average loss at step 500 = 12.1282441187 \n",
      "learning rate: 7.49894\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 9.33\n",
      "------------------------------\n",
      "Average loss at step 1000 = 10.5865788126 \n",
      "learning rate: 5.62341\n",
      "Minibatch perplexity: 6.93\n",
      "================================================================================\n",
      "vglenfne nuner parkytains infor tow lassixsilly by two wor q of from light conisc\n",
      "pcunt was the dive hom locoecresnwn one ne seven two greessuoi and three fius tr \n",
      "mor concyc and couing two  irge the les storirions zero the cen alle ne the puman\n",
      "zly ten inned qwbel one untatito the decokang a for three binknop with onare is o\n",
      "ssed the compulobition the mact four whill setts of idpihs the the the beartatary\n",
      "================================================================================\n",
      "Validation set perplexity: 8.46\n",
      "------------------------------\n",
      "Average loss at step 1500 = 10.2594826102 \n",
      "learning rate: 4.21697\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 8.14\n",
      "------------------------------\n",
      "Average loss at step 2000 = 10.072661463 \n",
      "learning rate: 3.16228\n",
      "Minibatch perplexity: 5.65\n",
      "================================================================================\n",
      "kdg conal schistinfor is the word maekps as of gor ocienzwing in and holter of th\n",
      "lqsas whistion a the meater hode groms with the piltiona u same one zero king le \n",
      "sr one a by one sone zero eight four eight nine hin bisholly which he time cons t\n",
      "xmlly eominary gitexwsnb one foun ad four mupune liven and rllaty thes on icpusts\n",
      "yyonme is five and the mose wargan the the chowences pruoaci gente phor by auqpsn\n",
      "================================================================================\n",
      "Validation set perplexity: 7.91\n",
      "------------------------------\n",
      "Average loss at step 2500 = 9.99082185745 \n",
      "learning rate: 2.37137\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 7.59\n",
      "------------------------------\n",
      "Average loss at step 3000 = 9.90465206504 \n",
      "learning rate: 1.77828\n",
      "Minibatch perplexity: 6.24\n",
      "================================================================================\n",
      "ies five three nine nine the zero one seven sauilister chrale s pufse inderator l\n",
      "gy ricpihen prongebrand and marses the ands the avbto kbassed t mesing in wory if\n",
      "ps and accan ran letial tive triky to as aracygcqd of as to side from witists sch\n",
      "gbgzero a and puliponce cludon anced one nine nine eight six two two zero in the \n",
      "zkon bution hamppeuted to demcowa motical six five cher jlary the at and with hhm\n",
      "================================================================================\n",
      "Validation set perplexity: 7.47\n",
      "------------------------------\n",
      "Average loss at step 3500 = 9.9732060504 \n",
      "learning rate: 1.33352\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.47\n",
      "------------------------------\n",
      "Average loss at step 4000 = 9.88552076221 \n",
      "learning rate: 1.0\n",
      "Minibatch perplexity: 7.32\n",
      "================================================================================\n",
      "jt in one five two zero zero dibund amerier counded cert and of the by dies bald \n",
      "rchic of the unirac the losic mawely presned into a prent of muted there the ameg\n",
      "hat used his wedlain which complon six sent unish at comment doalleg eight st of \n",
      "tqdnals one four zero two nine one eight four zero zero one nine one nine eight o\n",
      "jylcourocicaliies of thery the cc the eagerre two one to comphased the gem is kin\n",
      "================================================================================\n",
      "Validation set perplexity: 7.35\n",
      "------------------------------\n",
      "Average loss at step 4500 = 9.86823533654 \n",
      "learning rate: 0.749894\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.33\n",
      "------------------------------\n",
      "Average loss at step 5000 = 9.81179987788 \n",
      "learning rate: 0.562341\n",
      "Minibatch perplexity: 6.01\n",
      "================================================================================\n",
      " us bhg incoguates bolle at and pole decent to empimoluyers two s and preskely sh\n",
      " langoan weskettly one eight nine folr four wenling cunistrahe might eight two th\n",
      "o jude far and sor dom conglish diporgit adepoids wouch sxationand base for cohst\n",
      "obmothan counts and s uniting and the is the econtr saw for contonot one synt iso\n",
      "ltions emh caubor word of notings the aloc to the such most elolute fitle the sta\n",
      "================================================================================\n",
      "Validation set perplexity: 7.24\n",
      "------------------------------\n",
      "0:04:06 elapsed time\n"
     ]
    }
   ],
   "source": [
    "# training and validation batches\n",
    "train_batches = BatchGeneratorBigram(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGeneratorBigram(valid_text, 1, 2) # returns batch size 1, +2 unrolling\n",
    "train_batches_y = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches_y = BatchGenerator(valid_text, 1, 2) # returns batch size 1, +2 unrolling \n",
    "\n",
    "num_steps = 5001  ## orig 7001\n",
    "summary_frequency = 100\n",
    "keep_prob = 0.5\n",
    "\n",
    "t0 = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print( 'Initialized\\n==========')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    batches_y = train_batches_y.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]  # feed input data\n",
    "      feed_dict[train_data_y[i]] = batches_y[i]  # feed vectorized label data\n",
    "    feed_dict[keep_prob] = keep_prob  # dropout during training\n",
    "    \n",
    "    # evaluate graph\n",
    "    _, l, lr = session.run([optimizer, loss, learning_rate], feed_dict=feed_dict)\n",
    "    \n",
    "    mean_loss += l\n",
    "    if step % (5.*summary_frequency) == 0:  ## orig 2.5*summary_frequency\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print( 'Average loss at step', step, '=', mean_loss, '\\nlearning rate:', lr)\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches_y)[2:])  # offset labels for bigram\n",
    "      feed_dict[keep_prob] = 1.\n",
    "      predictions = train_prediction.eval(feed_dict=feed_dict)  # predict w/out dropout\n",
    "      print( 'Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print( '=' * 80)\n",
    "        for _ in range(5):\n",
    "          c_1 = id2char(np.random.randint(27, size=[1]))\n",
    "          c_2 = id2char(np.random.randint(27, size=[1]))\n",
    "          feed = np.array([dictionary[c_1+c_2]])  # for bigram model\n",
    "          sentence = c_1 + c_2\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.})\n",
    "            pred_ohe = sample(prediction)  # get ohe of predicted proba\n",
    "            pred_c = id2char(np.argmax(pred_ohe))  # convert id of prediction\n",
    "            sentence += pred_c  # add predicted char\n",
    "            feed = np.array([dictionary[c_2 + pred_c]])\n",
    "            c_2 = pred_c\n",
    "          print( sentence)\n",
    "        print( '=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        b_y = valid_batches_y.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1.})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b_y[2])\n",
    "      print( 'Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n",
    "      print( '-' * 30)\n",
    "        \n",
    "# show how much time elapsed\n",
    "m, s = divmod(time.time()-t0, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print(\"%d:%02d:%02d\" % (h, m, s), 'elapsed time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists adv', 'on from the nation', 'significant than i', 'ain drugs confusio', 'ate of the origina', 't or at least not ', 'he first daily col', 'rdoo ricky ricardo']\n",
      "['dvocate social rel', 'onal media and fro', ' in jersey and gue', 'ion inability to o', 'nal document fax m', 't parliament s opp', 'ollege newspaper i', 'do this classic in']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "bigram_vocabulary_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size_in_chars = len(text)\n",
    "        self._text_size = self._text_size_in_chars // 2\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=self._batch_size, dtype=np.int)\n",
    "        for b in range(self._batch_size):\n",
    "            char_idx = self._cursor[b] * 2\n",
    "            ch1 = char2id(self._text[char_idx])\n",
    "            if self._text_size_in_chars - 1 == char_idx:\n",
    "                ch2 = 0\n",
    "            else:\n",
    "                ch2 = char2id(self._text[char_idx + 1])\n",
    "            batch[b] = ch1 * vocabulary_size + ch2\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "\n",
    "def bi2str(encoding):\n",
    "    return id2char(encoding // vocabulary_size) + id2char(encoding % vocabulary_size)\n",
    "\n",
    "\n",
    "def bigrams(encodings):\n",
    "    return [bi2str(e) for e in encodings]\n",
    "\n",
    "\n",
    "def bibatches2string(batches):\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, bigrams(b))]\n",
    "    return s\n",
    "\n",
    "\n",
    "bi_onehot = np.zeros((bigram_vocabulary_size, bigram_vocabulary_size))\n",
    "np.fill_diagonal(bi_onehot, 1)\n",
    "\n",
    "\n",
    "def bi_one_hot(encodings):\n",
    "    return [bi_onehot[e] for e in encodings]\n",
    "\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_text, 8, 8)\n",
    "valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(bibatches2string(train_batches.next()))\n",
    "print(bibatches2string(train_batches.next()))\n",
    "print(bibatches2string(valid_batches.next()))\n",
    "print(bibatches2string(valid_batches.next()))\n",
    "\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "\n",
    "def sample(prediction, size=vocabulary_size):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def one_hot_voc(prediction, size=vocabulary_size):\n",
    "    p = np.zeros(shape=[1, size], dtype=np.float)\n",
    "    p[0, prediction[0]] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def random_distribution(size=vocabulary_size):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, size])\n",
    "    return b / np.sum(b, 1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.735011 learning rate: 10.000000\n",
      "Minibatch perplexity: 841.35\n",
      "================================================================================\n",
      "fos hezjsos fjhzs wkkus cysts hss hse ewzqips qccyxrs olsakgs ddelags zdxps wglxjovds ltzos omcollse\n",
      "mulwsrs yte tasmyqajghlraipr cbis eltwuepae egs oos t kbdfs gllts ntggesstpmonbys xnsps znwcicepcmda\n",
      "rxgqs vps wyyos pbs gjloe  gats uuuwvfednhs ufezwhs igqhs ibsnjsyzs dls jrs phs rhu s wtiys raess vr\n",
      "cus yvyes zdbrs ros ixmxaltns kgcos pymgs yydzprs uenos wk ns  cyfonmls pss jhpjsbs bxkjs sapee vms \n",
      "tkicyqs dpolnmcxs ulmjdezvq s ixoas qgjyrxnxhfs qcaeqks mmqyb s f fhjbxwldlsuss dpntykf ijkis cmuz f\n",
      "================================================================================\n",
      "Validation set perplexity: 2999.03\n",
      "Average loss at step 100: 6.065820 learning rate: 10.000000\n",
      "Minibatch perplexity: 151.44\n",
      "Validation set perplexity: 120.97\n",
      "Average loss at step 200: 4.639520 learning rate: 10.000000\n",
      "Minibatch perplexity: 83.41\n",
      "Validation set perplexity: 84.08\n",
      "Average loss at step 300: 4.343146 learning rate: 10.000000\n",
      "Minibatch perplexity: 57.59\n",
      "Validation set perplexity: 64.83\n",
      "Average loss at step 400: 4.182585 learning rate: 10.000000\n",
      "Minibatch perplexity: 88.01\n",
      "Validation set perplexity: 53.94\n",
      "Average loss at step 500: 4.161264 learning rate: 9.000000\n",
      "Minibatch perplexity: 61.34\n",
      "Validation set perplexity: 49.91\n",
      "Average loss at step 600: 3.975612 learning rate: 9.000000\n",
      "Minibatch perplexity: 54.43\n",
      "Validation set perplexity: 43.58\n",
      "Average loss at step 700: 3.972810 learning rate: 9.000000\n",
      "Minibatch perplexity: 48.98\n",
      "Validation set perplexity: 42.77\n",
      "Average loss at step 800: 3.885701 learning rate: 9.000000\n",
      "Minibatch perplexity: 52.24\n",
      "Validation set perplexity: 37.59\n",
      "Average loss at step 900: 3.795788 learning rate: 9.000000\n",
      "Minibatch perplexity: 40.46\n",
      "Validation set perplexity: 36.04\n",
      "Average loss at step 1000: 3.758167 learning rate: 8.099999\n",
      "Minibatch perplexity: 47.93\n",
      "================================================================================\n",
      "gzy a urhubcground by with that loning and eng he dene world perort and zero four jpnnhold oachnhist\n",
      "mws are modent is poliupine  ment and reves hamart to two inayorhne it itscks grapformable whipforhr\n",
      "ey any eight nine eight and berch and ementainetnmencey enchessterg endiceed alwo beits jo kwobrone \n",
      "nk o mong far exger serist word archis mage lato in the ukazmment and comperagest eight four the cic\n",
      "mmen whichlepdmhdsset onrough the wests its screre in to b continuh comedaslow one eight six eight s\n",
      "================================================================================\n",
      "Validation set perplexity: 36.32\n",
      "Average loss at step 1100: 3.781434 learning rate: 8.099999\n",
      "Minibatch perplexity: 38.37\n",
      "Validation set perplexity: 33.76\n",
      "Average loss at step 1200: 3.775223 learning rate: 8.099999\n",
      "Minibatch perplexity: 38.40\n",
      "Validation set perplexity: 30.96\n",
      "Average loss at step 1300: 3.824703 learning rate: 8.099999\n",
      "Minibatch perplexity: 58.66\n",
      "Validation set perplexity: 28.61\n",
      "Average loss at step 1400: 3.784722 learning rate: 8.099999\n",
      "Minibatch perplexity: 39.66\n",
      "Validation set perplexity: 27.77\n",
      "Average loss at step 1500: 3.763988 learning rate: 7.289999\n",
      "Minibatch perplexity: 44.71\n",
      "Validation set perplexity: 27.93\n",
      "Average loss at step 1600: 3.741805 learning rate: 7.289999\n",
      "Minibatch perplexity: 35.09\n",
      "Validation set perplexity: 26.32\n",
      "Average loss at step 1700: 3.718299 learning rate: 7.289999\n",
      "Minibatch perplexity: 33.66\n",
      "Validation set perplexity: 26.18\n",
      "Average loss at step 1800: 3.728188 learning rate: 7.289999\n",
      "Minibatch perplexity: 32.75\n",
      "Validation set perplexity: 26.57\n",
      "Average loss at step 1900: 3.691685 learning rate: 7.289999\n",
      "Minibatch perplexity: 40.84\n",
      "Validation set perplexity: 26.39\n",
      "Average loss at step 2000: 3.665281 learning rate: 6.560999\n",
      "Minibatch perplexity: 35.42\n",
      "================================================================================\n",
      "sq sometic refovordo zero zero five one nine four nine six five zero five seven neula unhylastoyion \n",
      "p of the namism floam ormediestion hiswo one one and revely was at where ilurezu by and upperst plat\n",
      "graplation liberial pop was to and smyiers bright greganian its of the numbers forsas of the sucder \n",
      "ections have procal world his ecific with between agation the n bookcian notally boundingdomen to of\n",
      "etic of the unation thembers by reures stand the yeaum is into medahir one zero zero zero calleded p\n",
      "================================================================================\n",
      "Validation set perplexity: 25.90\n",
      "Average loss at step 2100: 3.659397 learning rate: 6.560999\n",
      "Minibatch perplexity: 38.97\n",
      "Validation set perplexity: 24.34\n",
      "Average loss at step 2200: 3.624272 learning rate: 6.560999\n",
      "Minibatch perplexity: 47.30\n",
      "Validation set perplexity: 24.77\n",
      "Average loss at step 2300: 3.583837 learning rate: 6.560999\n",
      "Minibatch perplexity: 29.46\n",
      "Validation set perplexity: 24.98\n",
      "Average loss at step 2400: 3.650109 learning rate: 6.560999\n",
      "Minibatch perplexity: 33.20\n",
      "Validation set perplexity: 24.49\n",
      "Average loss at step 2500: 3.622399 learning rate: 5.904899\n",
      "Minibatch perplexity: 28.18\n",
      "Validation set perplexity: 24.77\n",
      "Average loss at step 2600: 3.637325 learning rate: 5.904899\n",
      "Minibatch perplexity: 39.46\n",
      "Validation set perplexity: 24.20\n",
      "Average loss at step 2700: 3.579021 learning rate: 5.904899\n",
      "Minibatch perplexity: 36.47\n",
      "Validation set perplexity: 23.72\n",
      "Average loss at step 2800: 3.542010 learning rate: 5.904899\n",
      "Minibatch perplexity: 44.69\n",
      "Validation set perplexity: 23.92\n",
      "Average loss at step 2900: 3.571897 learning rate: 5.904899\n",
      "Minibatch perplexity: 45.04\n",
      "Validation set perplexity: 23.68\n",
      "Average loss at step 3000: 3.524921 learning rate: 5.314409\n",
      "Minibatch perplexity: 28.96\n",
      "================================================================================\n",
      "yyy frenshas bies bove one cears a constital former to reverall nine six four zero zero zero zero fo\n",
      "und recordposet of the democcuble ejrophishant socian is direct aim force zero four eight english se\n",
      "rhef addayed its ofte up leg havarnt alfil here three and repoke ws extrality at smithers is six eig\n",
      "ad la ty juster more formars antelected steewrith a in one nine two zero four three six four nine th\n",
      "zm womks bashane the long the d majorran ransool instrolity catch johned indense rancept divince for\n",
      "================================================================================\n",
      "Validation set perplexity: 22.94\n",
      "Average loss at step 3100: 3.514184 learning rate: 5.314409\n",
      "Minibatch perplexity: 30.22\n",
      "Validation set perplexity: 23.08\n",
      "Average loss at step 3200: 3.558599 learning rate: 5.314409\n",
      "Minibatch perplexity: 35.60\n",
      "Validation set perplexity: 23.28\n",
      "Average loss at step 3300: 3.582409 learning rate: 5.314409\n",
      "Minibatch perplexity: 40.98\n",
      "Validation set perplexity: 23.36\n",
      "Average loss at step 3400: 3.568301 learning rate: 5.314409\n",
      "Minibatch perplexity: 30.20\n",
      "Validation set perplexity: 23.24\n",
      "Average loss at step 3500: 3.493801 learning rate: 4.782968\n",
      "Minibatch perplexity: 30.93\n",
      "Validation set perplexity: 22.46\n",
      "Average loss at step 3600: 3.464457 learning rate: 4.782968\n",
      "Minibatch perplexity: 24.76\n",
      "Validation set perplexity: 22.91\n",
      "Average loss at step 3700: 3.449510 learning rate: 4.782968\n",
      "Minibatch perplexity: 32.07\n",
      "Validation set perplexity: 21.78\n",
      "Average loss at step 3800: 3.421278 learning rate: 4.782968\n",
      "Minibatch perplexity: 33.38\n",
      "Validation set perplexity: 21.45\n",
      "Average loss at step 3900: 3.442780 learning rate: 4.782968\n",
      "Minibatch perplexity: 33.27\n",
      "Validation set perplexity: 22.35\n",
      "Average loss at step 4000: 3.529123 learning rate: 4.304671\n",
      "Minibatch perplexity: 36.92\n",
      "================================================================================\n",
      "oolsed home sust resoted voiijocal commentutism ratenxed younti and distthis the radtary fealidesput\n",
      "ron and zero even langings rost the monties unioned the moreughtes lou britifing from many oran terr\n",
      "prisous english the implack his to amoph national had voides state fasibution in the political lawd \n",
      "gjesticient moding dowcs is life stays storican laterwital kawoyle yan two three six five one four o\n",
      "ue ros mode in hyparch israen barbetween gods iiiatory b of relittained actters endieveral higntaid \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 22.50\n",
      "0:12:55 elapsed time\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 512\n",
    "num_unrollings = 10\n",
    "batch_size = 32\n",
    "embedding_size = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input to all gates\n",
    "    x = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1), name='x')\n",
    "    # memory of all gates\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1), name='m')\n",
    "    # biases all gates\n",
    "    biases = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_vocabulary_size]))\n",
    "    # embeddings for all possible bigrams\n",
    "    embeddings = tf.Variable(tf.random_uniform([bigram_vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # one hot encoding for labels in\n",
    "    np_one_hot = np.zeros((bigram_vocabulary_size, bigram_vocabulary_size))\n",
    "    np.fill_diagonal(np_one_hot, 1)\n",
    "    bigram_one_hot = tf.constant(np.reshape(np_one_hot, -1), dtype=tf.float32,\n",
    "                                 shape=[bigram_vocabulary_size, bigram_vocabulary_size])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        i = tf.nn.dropout(i, keep_prob)\n",
    "        mult = tf.matmul(i, x) + tf.matmul(o, m) + biases\n",
    "        input_gate = tf.sigmoid(mult[:, :num_nodes])\n",
    "        forget_gate = tf.sigmoid(mult[:, num_nodes:num_nodes * 2])\n",
    "        update = mult[:, num_nodes * 3:num_nodes * 4]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(mult[:, num_nodes * 3:])\n",
    "        output = tf.nn.dropout(output_gate * tf.tanh(state), keep_prob)\n",
    "        return output, state\n",
    "\n",
    "\n",
    "    # Input data. [num_unrollings, batch_size] -> one hot encoding removed, we send just bigram ids\n",
    "    tf_train_data = tf.placeholder(tf.int32, shape=[num_unrollings + 1, batch_size])\n",
    "    train_data = list()\n",
    "    for i in tf.split(tf_train_data, num_unrollings + 1, 0):\n",
    "        train_data.append(tf.squeeze(i))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = list()\n",
    "    for l in train_data[1:]:\n",
    "        train_labels.append(tf.gather(bigram_one_hot, l))\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    # python loop used: tensorflow does not support sequential operations yet\n",
    "    for i in train_inputs:  # having a loop simulates having time\n",
    "        # embed input bigrams -> [batch_size, embedding_size]\n",
    "        output, state = lstm_cell(tf.nn.embedding_lookup(embeddings, i), output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings, control_dependencies makes sure that output and state are computed\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,0),logits=logits))\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 500, 0.9, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # here we predict the embedding\n",
    "    # train_prediction = tf.argmax(tf.nn.softmax(logits), 1, name='train_prediction')\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    embed_sample_input = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    sample_output, sample_state = lstm_cell(embed_sample_input, saved_sample_output, saved_sample_state)\n",
    "\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\n",
    "num_steps = 4001\n",
    "summary_frequency = 100\n",
    "# initalize batch generators\n",
    "t0=time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run() \n",
    "    print('Initialized')\n",
    "    train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "    valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        _, l, lr, predictions = session.run([optimizer, loss, learning_rate, train_prediction],\n",
    "                                            feed_dict={tf_train_data: batches, keep_prob: 0.6})\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = list(batches)[1:]\n",
    "            labels = np.concatenate([bi_one_hot(l) for l in labels])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = np.argmax(sample(random_distribution(bigram_vocabulary_size), bigram_vocabulary_size))\n",
    "                    sentence = bi2str(feed)\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(49):\n",
    "                        prediction = sample_prediction.eval({sample_input: [feed], keep_prob: 1.0})\n",
    "                        feed = np.argmax(sample(prediction, bigram_vocabulary_size))\n",
    "                        sentence += bi2str(feed)\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1.0})\n",
    "                # print(predictions)\n",
    "                valid_logprob = valid_logprob + logprob(predictions, one_hot_voc(b[1], bigram_vocabulary_size))\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n",
    "            \n",
    "# show how much time elapsed\n",
    "m, s = divmod(time.time()-t0, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print(\"%d:%02d:%02d\" % (h, m, s), 'elapsed time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mirrored text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) by count:\n",
      "[['UNK', 315138], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430), ('two', 192644), ('is', 183153), ('as', 131815), ('eight', 125285), ('for', 118445)]\n",
      "Sample data (indexes of words):\n",
      "[5239, 3081, 12, 6, 195, 2, 3135, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "# build a dictionary and replace rare words\n",
    "import collections\n",
    "\n",
    "vocabulary_size = 2**16  # orig 50000\n",
    "words = text.split()\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()  # word: int\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))  # int: word\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print( 'Most common words (+UNK) by count:')\n",
    "print( count[:15])\n",
    "print( 'Sample data (indexes of words):')\n",
    "print( data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= ons anarchists advo\n",
      "y= sno stsihcrana ovda\n",
      "global step 100 learning rate 1.0000 perplexity 16.82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d4be0019f9e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# Get a batch and make a step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_sets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstep_ckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivanchen/Data-Science/github/ud730/seq2seq_model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0moutput_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Gradient norm, loss, no outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivanchen/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivanchen/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivanchen/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivanchen/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivanchen/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import seq2seq_model\n",
    "\n",
    "import math\n",
    "\n",
    "batch_size = 64\n",
    "num_unrollings = 19\n",
    "\n",
    "\n",
    "class Seq2SeqBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // num_unrollings\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch(0)\n",
    "\n",
    "    def _next_batch(self, step):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = ''\n",
    "        # print('text size', self._text_size)\n",
    "        for b in range(self._num_unrollings):\n",
    "            # print(self._cursor[step])\n",
    "            self._cursor[step] %= self._text_size\n",
    "            batch += self._text[self._cursor[step]]\n",
    "            self._cursor[step] += 1\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._batch_size):\n",
    "            batches.append(self._next_batch(step))\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def ids(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [str(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2id(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, ids(b))]\n",
    "    return s\n",
    "\n",
    "\n",
    "train_batches = Seq2SeqBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = Seq2SeqBatchGenerator(valid_text, 1, num_unrollings)\n",
    "\n",
    "\n",
    "def rev_id(forward):\n",
    "    temp = forward.split(' ')\n",
    "    backward = []\n",
    "    for i in range(len(temp)):\n",
    "        backward += temp[i][::-1] + ' '\n",
    "    return list(map(lambda x: char2id(x), backward[:-1]))\n",
    "\n",
    "\n",
    "batches = train_batches.next()\n",
    "train_sets = []\n",
    "batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "batch_decs = list(map(lambda x: rev_id(x), batches))\n",
    "print('x=', ''.join([id2char(x) for x in batch_encs[0]]))\n",
    "print('y=', ''.join([id2char(x) for x in batch_decs[0]]))\n",
    "\n",
    "\n",
    "def create_model(forward_only):\n",
    "    model = seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n",
    "                                       target_vocab_size=vocabulary_size,\n",
    "                                       buckets=[(20, 20)],\n",
    "                                       size=256,\n",
    "                                       num_layers=4,\n",
    "                                       max_gradient_norm=5.0,\n",
    "                                       batch_size=batch_size,\n",
    "                                       learning_rate=1.0,\n",
    "                                       learning_rate_decay_factor=0.9,\n",
    "                                       use_lstm=True,\n",
    "                                       forward_only=forward_only)\n",
    "    return model\n",
    "\n",
    "t0 = time.time()\n",
    "with tf.Session() as sess:\n",
    "    model = create_model(False)\n",
    "    #sess.run(tf.initialize_all_variables())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_steps = 30001\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    step_ckpt = 100\n",
    "    valid_ckpt = 500\n",
    "\n",
    "    for step in range(1, num_steps):\n",
    "        model.batch_size = batch_size\n",
    "        batches = train_batches.next()\n",
    "        train_sets = []\n",
    "        batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "        batch_decs = list(map(lambda x: rev_id(x), batches))\n",
    "        for i in range(len(batch_encs)):\n",
    "            train_sets.append((batch_encs[i], batch_decs[i]))\n",
    "\n",
    "        # Get a batch and make a step.\n",
    "        encoder_inputs, decoder_inputs, target_weights = model.get_batch([train_sets], 0)\n",
    "        _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, False)\n",
    "\n",
    "        loss += step_loss / step_ckpt\n",
    "\n",
    "        # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "        if step % step_ckpt == 0:\n",
    "            # Print statistics for the previous epoch.\n",
    "            perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "            print(\"global step %d learning rate %.4f perplexity \"\n",
    "                  \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(), perplexity))\n",
    "            # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "            if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                sess.run(model.learning_rate_decay_op)\n",
    "            previous_losses.append(loss)\n",
    "\n",
    "            loss = 0.0\n",
    "\n",
    "            if step % valid_ckpt == 0:\n",
    "                v_loss = 0.0\n",
    "\n",
    "                model.batch_size = 1\n",
    "                batches = ['the quick brown fox']\n",
    "                test_sets = []\n",
    "                batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "                # batch_decs = map(lambda x: rev_id(x), batches)\n",
    "                test_sets.append((batch_encs[0], []))\n",
    "                # Get a 1-element batch to feed the sentence to the model.\n",
    "                encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n",
    "                # Get output logits for the sentence.\n",
    "                _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "\n",
    "                # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "                outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "\n",
    "                print('>>>>>>>>> ', batches[0], ' -> ', ''.join(map(lambda x: id2char(x), outputs)))\n",
    "\n",
    "                for _ in range(valid_size):\n",
    "                    model.batch_size = 1\n",
    "                    v_batches = valid_batches.next()\n",
    "                    valid_sets = []\n",
    "                    v_batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), v_batches))\n",
    "                    v_batch_decs = list(map(lambda x: rev_id(x), v_batches))\n",
    "                    for i in range(len(v_batch_encs)):\n",
    "                        valid_sets.append((v_batch_encs[i], v_batch_decs[i]))\n",
    "                    encoder_inputs, decoder_inputs, target_weights = model.get_batch([valid_sets], 0)\n",
    "                    _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "                    v_loss += eval_loss / valid_size\n",
    "\n",
    "                eval_ppx = math.exp(v_loss) if v_loss < 300 else float('inf')\n",
    "                print(\"  valid eval:  perplexity %.2f\" % (eval_ppx))\n",
    "\n",
    "    # reuse variable -> subdivide into two boxes\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "    batches = ['the quick brown fox']\n",
    "    test_sets = []\n",
    "    batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "    # batch_decs = map(lambda x: rev_id(x), batches)\n",
    "    test_sets.append((batch_encs[0], []))\n",
    "    # Get a 1-element batch to feed the sentence to the model.\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n",
    "    # Get output logits for the sentence.\n",
    "    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "    # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "    print('## : ', outputs)\n",
    "    # If there is an EOS symbol in outputs, cut them at that point.\n",
    "    if char2id('!') in outputs:\n",
    "        outputs = outputs[:outputs.index(char2id('!'))]\n",
    "\n",
    "    print(batches[0], ' -> ', ''.join(map(lambda x: id2char(x), outputs)))\n",
    "    \n",
    "# show how much time elapsed\n",
    "m, s = divmod(time.time()-t0, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print(\"%d:%02d:%02d\" % (h, m, s), 'elapsed time')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
